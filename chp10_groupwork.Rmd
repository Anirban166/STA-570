---
title: "Chapter 10 Homework by Anirban Chetia"
author: "Brit Riggs, Anirban Chetia"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---
  
## Reflective Notes  
  
I think I got everything correct, i.e. its a '\textbf{GOT IT}' from my side for all the questions.  
  
```{r, message = FALSE, warning = FALSE, echo = FALSE}
suppressPackageStartupMessages({
 library(dplyr)
 library(knitr)  
 library(ggplot2)
 library(ggfortify)
 library(latex2exp)
})
```

# Exercise 1

## 1(a)

```{r}
p1.data <- data.frame(x = c(3, 8, 10, 18, 23, 28), y = c(14, 28, 43, 62, 79, 86))
ggplot(p1.data, aes(x = x, y = y)) + geom_point()
```

## 1(b)

### (i)

```{r}
xbar <- mean(p1.data$x)
ybar <- mean(p1.data$y)
sx <- sd(p1.data$x)
sy <- sd(p1.data$y)
x <- p1.data$x
y <- p1.data$y
n <- length(x)
Sxx <- sum((x - mean(x))^2)
r <- sum((x - mean(x))/sd(x)*(y-mean(y))/sd(y))/(n-1)
cbind(xbar, ybar, sx = round(sx, 2), sy = round(sy, 2), Sxx, r = round(r, 4))
```

As can be observed from above, the values I obtained by the use of R are the exact same as the ones from the summary statistics. (hence, confirmed)

### (ii)

![](./1(b)(2).jpg)

```{r}
beta1 <- r * sy / sx
beta0 <- ybar - beta1 * xbar
cbind(beta1, beta0)
```

### (iii)

![](./1(b)(3).jpg)

```{r}
(yhat <- beta0 + beta1 * p1.data$x)
```

### (iv)

![](./1(b)(4).jpg)

```{r}
(epsilonhat <- p1.data$y - yhat)
```

### (v)

![](./1(b)(5).jpg)

```{r}
SSE <- sum(epsilonhat^2)
n <- length(p1.data$x)
MSE <- SSE/(n - 2)
sigmahat <- sqrt(MSE)
cbind(SSE, MSE, sigmahat)
```

### (vi)

![](./1(b)(6).jpg)

```{r}
SEbeta0hat <- sigmahat * sqrt(1 / n + xbar^2 / Sxx)
SEbeta1hat <- sigmahat * sqrt(1 / Sxx)
cbind(SEbeta0hat, SEbeta1hat)
```

## 1(c)

```{r}
model <- lm(y ~ x, p1.data)
```

### (i)

```{r}
predict(model)
```

### (ii)

```{r}
resid(model)
# or model$residuals
```

### (iii)

```{r}
summary(model)
```

## 1(d)

```{r}
confint(model)
```

The 95% CI for $\hat{\beta}_{1}$ is 2.36 to 3.54.

## 1(e)

$$ H_0 : \beta_1 = 0 $$
$$ H_a : \beta_1 \ne 0 $$

```{r}
(result <- summary(model))
```

Based on the p-value obtained (0.000155) for the x term, we reject the null hypothesis $H_o$ that $\beta_1$ = 0, and are in favor of $H_a$ ($\beta_1 \ne 0$), concluding that there is a non-constant linear relationship between x and y.

## 1(f)

Thus, the R-squared value for this regression is:
$$
R^2 = \frac{SS_{diff}}{SST} = \frac{4003.2}{4003.2 + 82.9} = 0.9797117
$$
```{r}
result$r.squared
```

## 1(g)

The typical distance to the regression line (for a considered data point) is $\hat{\sigma}$, which is 4.55, as I obtained from my calculations.
```{r}
result$sigma
```

## 1(h)

```{r}
ggplot(p1.data, aes(x = x, y = y)) + geom_point() + geom_smooth(method = 'lm')
```

# Exercise 2

## 2(a)

![](./2(a).jpg)

\newpage

# Exercise 3

## 3(a)

### (i)

```{r}
p3.data <- data.frame(Dose = c(2, 2, 2, 4, 4, 8, 8, 16, 16, 16, 32, 32, 64, 64, 64), 
                      Response = c(5, 7, 3, 10, 14, 15, 17, 20, 21, 19, 23, 29, 28, 31, 30))
ggplot(p3.data, aes(x = Dose, y = Response)) + geom_point()
```

The relationship in between the dose and response looks like a logarithmic or square-root trend.

### (ii)

```{r}
(linear.model <- lm(Response ~ Dose, data = p3.data))
```

### (iii)

```{r}
problem.check <- data.frame(residuals = resid(linear.model), prediction = predict(linear.model))
ggplot(problem.check, aes(x = prediction, y = residuals)) + geom_point()
```

Examining the plot of residuals vs fitted values, I can observe that the residual variance near predicted values of 0 appears larger than the variance elsewhere and the residuals are positive for values at the middle (of prediction) but negative on the extremes, so linear regression is doubly inappropriate.

\newpage

## 3(b)

### (i)

```{r}
p3.data <- p3.data %>% mutate(logdose = log(Dose))
ggplot(p3.data, aes(x = logdose, y = Response)) + geom_point()
```

Yes, it indeed does appear that linear regression would be appropriate here.

### (ii)

```{r}
log.linear.model <- lm(Response ~ logdose, data = p3.data)
logerrorcheck <- data.frame(log.residuals = resid(log.linear.model), log.prediction = predict(log.linear.model))
```

### (iii)

```{r}
ggplot(logerrorcheck, aes(x = log.prediction, y = log.residuals)) + geom_point()
```

Yes,the linear model does seem to be appropriate!

### (iv)

```{r}
qqnorm(logerrorcheck$log.residuals)
shapiro.test(logerrorcheck$log.residuals)
```

The QQ plot looks okay for most of the part, but tends to seem a bit inappropriate with oddly large quantiles at the upper end. The Shapiro-Wilk normality test gave a p-value of 0.6011, which indicates that the deviation from normality may not be too big of a deal.

### (v)

```{r}
(log.slope <- log.linear.model$coefficients[2])
```

Given that the slope from the regression is 7.009967, every unit change in `log(dose)` will be accompanied by that much (value) of a unit change in the response.

### (vi)

95% CI for the regression parameters (y-intercept and slope) are:

```{r}
confint(log.linear.model)
```
Given that the interval for logdose's parameter doesn't contain 0, `log(dose)` is indeed a statistically significant predictor of the response. 
